{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mKZt6U9HGQ3f",
        "outputId": "5bfb9083-48d5-42fb-8e53-282ff874d00b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in e:\\eil\\embeddings\\.venv\\lib\\site-packages (2.7.1)\n",
            "Requirement already satisfied: tensorboard in e:\\eil\\embeddings\\.venv\\lib\\site-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (2.2.6)\n",
            "Requirement already satisfied: packaging in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (6.31.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (65.5.0)\n",
            "Requirement already satisfied: six>1.9 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6UMCc_nSgP9",
        "outputId": "c6cf5b73-1796-485d-8699-3863018cd5bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyMuPDF in e:\\eil\\embeddings\\.venv\\lib\\site-packages (1.26.3)\n",
            "Requirement already satisfied: sentence-transformers in e:\\eil\\embeddings\\.venv\\lib\\site-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d0Msb0DGHKfu",
        "outputId": "70f5d979-bc54-4d34-d2d4-6d14af64ab60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in e:\\eil\\embeddings\\.venv\\lib\\site-packages (5.0.0)\n",
            "Requirement already satisfied: datasets in e:\\eil\\embeddings\\.venv\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: transformers in e:\\eil\\embeddings\\.venv\\lib\\site-packages (4.53.0)\n",
            "Requirement already satisfied: tqdm in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (2.3.0)\n",
            "Requirement already satisfied: xxhash in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: sympy>=1.13.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers datasets transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332,
          "referenced_widgets": [
            "2d59225bbba7409caa89d1825fa5c0c1",
            "65d0240a89ae4e88a5d033637c52a061",
            "3a22eae0be5744aab43b0042d10f94ba",
            "e8d4e48bf7a646dd99e8c0c008f10655",
            "d5c64e8355b3426cae610b4498360047",
            "82a45c5911d64394935d929098227548",
            "b0442656fcea4423ba664d06307054a8",
            "aff6c8edede64a4087ea3509d3e5d3bc",
            "269340fb19a24356a6cfbab10ceed0cc",
            "74ac21c68edc454fa7acea4e879e377b",
            "6089020e1c9e40b799ed4bdd9e260d80",
            "4b11611a2090408fbee0a94616efb2d2",
            "2961efceeb1f4a1a8261bec721dee6ed",
            "048a59f82a774be7bb4f437603dd325b",
            "ce2e5768f2814f5d83a2afc8873607bd",
            "3cafc4c5f6874e25aafb6380fcdbd651",
            "e2d701ca633647a1ab1a75b6437726d5"
          ]
        },
        "id": "s5J1E-UWHvpf",
        "outputId": "7b9b4251-10e3-49a0-dccf-7df031345ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in e:\\eil\\embeddings\\.venv\\lib\\site-packages (8.1.7)\n",
            "Requirement already satisfied: comm>=0.1.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipywidgets) (8.37.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: colorama in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: decorator in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: exceptiongroup in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: jedi>=0.16 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
            "Requirement already satisfied: wcwidth in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: executing>=1.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc4ed14c1845482b84dc06b6ded2ae91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install ipywidgets\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OgA6sBh2IpaV",
        "outputId": "03d4474a-53f9-44d0-b398-29493a218942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in e:\\eil\\embeddings\\.venv\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (2.2.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: colorama in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsT1nN_2B9Wi",
        "outputId": "0b5817ec-d5b7-4dc0-d558-a2c4667d12cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in e:\\eil\\embeddings\\.venv\\lib\\site-packages (1.26.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in e:\\eil\\embeddings\\.venv\\lib\\site-packages (1.8.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from accelerate) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from accelerate) (2.7.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from accelerate) (0.33.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: colorama in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e:\\EIL\\embeddings\\.venv\\Scripts\\python.exe\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                 Version\n",
            "----------------------- -----------\n",
            "absl-py                 2.3.0\n",
            "accelerate              1.8.1\n",
            "aiohappyeyeballs        2.6.1\n",
            "aiohttp                 3.12.13\n",
            "aiosignal               1.3.2\n",
            "asttokens               3.0.0\n",
            "async-timeout           5.0.1\n",
            "attrs                   25.3.0\n",
            "bitsandbytes            0.46.1\n",
            "certifi                 2025.6.15\n",
            "charset-normalizer      3.4.2\n",
            "colorama                0.4.6\n",
            "comm                    0.2.2\n",
            "datasets                3.6.0\n",
            "debugpy                 1.8.14\n",
            "decorator               5.2.1\n",
            "dill                    0.3.8\n",
            "exceptiongroup          1.3.0\n",
            "executing               2.2.0\n",
            "filelock                3.18.0\n",
            "frozenlist              1.7.0\n",
            "fsspec                  2025.3.0\n",
            "grpcio                  1.73.1\n",
            "huggingface-hub         0.33.2\n",
            "idna                    3.10\n",
            "ipykernel               6.29.5\n",
            "ipython                 8.37.0\n",
            "ipywidgets              8.1.7\n",
            "jedi                    0.19.2\n",
            "Jinja2                  3.1.6\n",
            "joblib                  1.5.1\n",
            "jupyter_client          8.6.3\n",
            "jupyter_core            5.8.1\n",
            "jupyterlab_widgets      3.0.15\n",
            "Markdown                3.8.2\n",
            "MarkupSafe              3.0.2\n",
            "matplotlib-inline       0.1.7\n",
            "mpmath                  1.3.0\n",
            "multidict               6.6.3\n",
            "multiprocess            0.70.16\n",
            "nest-asyncio            1.6.0\n",
            "networkx                3.4.2\n",
            "numpy                   2.2.6\n",
            "packaging               25.0\n",
            "pandas                  2.3.0\n",
            "parso                   0.8.4\n",
            "pillow                  11.3.0\n",
            "pip                     25.1.1\n",
            "platformdirs            4.3.8\n",
            "prompt_toolkit          3.0.51\n",
            "propcache               0.3.2\n",
            "protobuf                6.31.1\n",
            "psutil                  7.0.0\n",
            "pure_eval               0.2.3\n",
            "pyarrow                 20.0.0\n",
            "Pygments                2.19.2\n",
            "PyMuPDF                 1.26.3\n",
            "python-dateutil         2.9.0.post0\n",
            "pytz                    2025.2\n",
            "pywin32                 310\n",
            "PyYAML                  6.0.2\n",
            "pyzmq                   27.0.0\n",
            "regex                   2024.11.6\n",
            "requests                2.32.4\n",
            "safetensors             0.5.3\n",
            "scikit-learn            1.7.0\n",
            "scipy                   1.15.3\n",
            "sentence-transformers   5.0.0\n",
            "setuptools              65.5.0\n",
            "six                     1.17.0\n",
            "stack-data              0.6.3\n",
            "sympy                   1.14.0\n",
            "tensorboard             2.19.0\n",
            "tensorboard-data-server 0.7.2\n",
            "threadpoolctl           3.6.0\n",
            "tokenizers              0.21.2\n",
            "torch                   2.7.1\n",
            "tornado                 6.5.1\n",
            "tqdm                    4.67.1\n",
            "traitlets               5.14.3\n",
            "transformers            4.53.0\n",
            "typing_extensions       4.14.0\n",
            "tzdata                  2025.2\n",
            "urllib3                 2.5.0\n",
            "wcwidth                 0.2.13\n",
            "Werkzeug                3.1.3\n",
            "widgetsnbextension      4.0.14\n",
            "xxhash                  3.5.0\n",
            "yarl                    1.20.1\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.8.1\n"
          ]
        }
      ],
      "source": [
        "import accelerate\n",
        "print(accelerate.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "from transformers.utils import is_accelerate_available\n",
        "print(is_accelerate_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.12.46-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl.metadata (439 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.2 (from llama_index)\n",
            "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.13,>=0.12.46 (from llama_index)\n",
            "  Downloading llama_index_core-0.12.46-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.7.8-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting nltk>3.8.1 (from llama_index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
            "  Downloading openai-1.93.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (3.12.13)\n",
            "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading banks-2.1.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (2025.3.0)\n",
            "Collecting httpx (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading llama_index_workflows-1.0.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (3.4.2)\n",
            "Requirement already satisfied: numpy in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (2.2.6)\n",
            "Requirement already satisfied: pillow>=9.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (11.3.0)\n",
            "Collecting pydantic>=2.8.0 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (2.32.4)\n",
            "Collecting setuptools>=80.9.0 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sqlalchemy>=1.4.49 (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.7.0 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading tiktoken-0.9.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13,>=0.12.46->llama_index) (4.14.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting wrapt (from llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading wrapt-1.17.2-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.46->llama_index) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama_index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama_index) (4.3.8)\n",
            "Collecting beautifulsoup4<5,>=4.12.3 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pandas<2.3.0 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
            "Collecting pypdf<6,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl.metadata (252 bytes)\n",
            "Collecting anyio<5,>=3.5.0 (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
            "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
            "  Downloading jiter-0.10.0-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting sniffio (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (3.10)\n",
            "Requirement already satisfied: certifi in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.46->llama_index) (2025.6.15)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: colorama in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.13,>=0.12.46->llama_index) (0.4.6)\n",
            "Collecting llama-cloud==0.1.30 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud-0.1.30-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.41-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.41 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.41-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting click<9.0.0,>=8.1.7 (from llama-cloud-services>=0.6.41->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.41->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: joblib in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.46->llama_index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.46->llama_index) (2.5.0)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached greenlet-3.2.3-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13,>=0.12.46->llama_index)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.46->llama_index) (25.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.46->llama_index) (3.0.2)\n",
            "Downloading llama_index-0.12.46-py3-none-any.whl (7.1 kB)\n",
            "Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.46-py3-none-any.whl (7.6 MB)\n",
            "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 1.6/7.6 MB 7.0 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 2.6/7.6 MB 6.6 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 3.9/7.6 MB 6.2 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 5.0/7.6 MB 5.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 6.3/7.6 MB 5.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 7.1/7.6 MB 5.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 7.6/7.6 MB 5.4 MB/s eta 0:00:00\n",
            "Downloading banks-2.1.3-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
            "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
            "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Downloading llama_index_workflows-1.0.1-py3-none-any.whl (36 kB)\n",
            "Downloading openai-1.93.0-py3-none-any.whl (755 kB)\n",
            "   ---------------------------------------- 0.0/755.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 755.0/755.0 kB 5.2 MB/s eta 0:00:00\n",
            "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading jiter-0.10.0-cp310-cp310-win_amd64.whl (207 kB)\n",
            "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
            "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 1.3/11.6 MB 6.7 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 2.6/11.6 MB 6.3 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 3.9/11.6 MB 6.2 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 5.0/11.6 MB 6.0 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 6.0/11.6 MB 5.8 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 7.1/11.6 MB 5.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 8.4/11.6 MB 5.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 9.7/11.6 MB 5.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.7/11.6 MB 5.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.5/11.6 MB 5.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.6/11.6 MB 5.4 MB/s eta 0:00:00\n",
            "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Using cached pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
            "Downloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.7.8-py3-none-any.whl (16 kB)\n",
            "Downloading llama_cloud-0.1.30-py3-none-any.whl (282 kB)\n",
            "Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_parse-0.6.41-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.41-py3-none-any.whl (40 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.5/1.5 MB 6.6 MB/s eta 0:00:00\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Using cached sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl (2.1 MB)\n",
            "Using cached greenlet-3.2.3-cp310-cp310-win_amd64.whl (296 kB)\n",
            "Downloading tiktoken-0.9.0-cp310-cp310-win_amd64.whl (894 kB)\n",
            "   ---------------------------------------- 0.0/894.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 894.0/894.0 kB 6.7 MB/s eta 0:00:00\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, wrapt, typing-inspection, tenacity, soupsieve, sniffio, setuptools, python-dotenv, pypdf, pydantic-core, mypy-extensions, marshmallow, jiter, h11, griffe, greenlet, distro, click, annotated-types, aiosqlite, typing-inspect, tiktoken, sqlalchemy, pydantic, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, llama-index-instrumentation, httpx, dataclasses-json, banks, openai, llama-index-workflows, llama-cloud, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "\n",
            "    ---------------------------------------  1/53 [filetype]\n",
            "    ---------------------------------------  1/53 [filetype]\n",
            "    ---------------------------------------  1/53 [filetype]\n",
            "   - --------------------------------------  2/53 [dirtyjson]\n",
            "   - --------------------------------------  2/53 [dirtyjson]\n",
            "   - --------------------------------------  2/53 [dirtyjson]\n",
            "   -- -------------------------------------  3/53 [wrapt]\n",
            "   --- ------------------------------------  5/53 [tenacity]\n",
            "   --- ------------------------------------  5/53 [tenacity]\n",
            "   ---- -----------------------------------  6/53 [soupsieve]\n",
            "   ----- ----------------------------------  7/53 [sniffio]\n",
            "  Attempting uninstall: setuptools\n",
            "   ----- ----------------------------------  7/53 [sniffio]\n",
            "    Found existing installation: setuptools 65.5.0\n",
            "   ----- ----------------------------------  7/53 [sniffio]\n",
            "    Uninstalling setuptools-65.5.0:\n",
            "   ----- ----------------------------------  7/53 [sniffio]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "      Successfully uninstalled setuptools-65.5.0\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  8/53 [setuptools]\n",
            "   ------ ---------------------------------  9/53 [python-dotenv]\n",
            "   ------ ---------------------------------  9/53 [python-dotenv]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   ------- -------------------------------- 10/53 [pypdf]\n",
            "   --------- ------------------------------ 13/53 [marshmallow]\n",
            "   --------- ------------------------------ 13/53 [marshmallow]\n",
            "   ----------- ---------------------------- 15/53 [h11]\n",
            "   ----------- ---------------------------- 15/53 [h11]\n",
            "   ------------ --------------------------- 16/53 [griffe]\n",
            "   ------------ --------------------------- 16/53 [griffe]\n",
            "   ------------ --------------------------- 16/53 [griffe]\n",
            "   ------------ --------------------------- 16/53 [griffe]\n",
            "   ------------ --------------------------- 16/53 [griffe]\n",
            "   ------------ --------------------------- 16/53 [griffe]\n",
            "   ------------ --------------------------- 16/53 [griffe]\n",
            "   ------------ --------------------------- 17/53 [greenlet]\n",
            "   ------------ --------------------------- 17/53 [greenlet]\n",
            "   ------------ --------------------------- 17/53 [greenlet]\n",
            "   ------------- -------------------------- 18/53 [distro]\n",
            "   -------------- ------------------------- 19/53 [click]\n",
            "   -------------- ------------------------- 19/53 [click]\n",
            "   --------------- ------------------------ 20/53 [annotated-types]\n",
            "   --------------- ------------------------ 21/53 [aiosqlite]\n",
            "   ----------------- ---------------------- 23/53 [tiktoken]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 24/53 [sqlalchemy]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "  Attempting uninstall: pandas\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "    Found existing installation: pandas 2.3.0\n",
            "   ------------------ --------------------- 25/53 [pydantic]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "    Uninstalling pandas-2.3.0:\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "      Successfully uninstalled pandas-2.3.0\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   ------------------- -------------------- 26/53 [pandas]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   -------------------- ------------------- 27/53 [nltk]\n",
            "   --------------------- ------------------ 28/53 [httpcore]\n",
            "   --------------------- ------------------ 28/53 [httpcore]\n",
            "   --------------------- ------------------ 28/53 [httpcore]\n",
            "   --------------------- ------------------ 28/53 [httpcore]\n",
            "   --------------------- ------------------ 29/53 [deprecated]\n",
            "   ---------------------- ----------------- 30/53 [beautifulsoup4]\n",
            "   ---------------------- ----------------- 30/53 [beautifulsoup4]\n",
            "   ---------------------- ----------------- 30/53 [beautifulsoup4]\n",
            "   ---------------------- ----------------- 30/53 [beautifulsoup4]\n",
            "   ---------------------- ----------------- 30/53 [beautifulsoup4]\n",
            "   ---------------------- ----------------- 30/53 [beautifulsoup4]\n",
            "   ----------------------- ---------------- 31/53 [anyio]\n",
            "   ----------------------- ---------------- 31/53 [anyio]\n",
            "   ----------------------- ---------------- 31/53 [anyio]\n",
            "   ----------------------- ---------------- 31/53 [anyio]\n",
            "   ------------------------ --------------- 32/53 [llama-index-instrumentation]\n",
            "   ------------------------ --------------- 32/53 [llama-index-instrumentation]\n",
            "   ------------------------ --------------- 32/53 [llama-index-instrumentation]\n",
            "   ------------------------ --------------- 33/53 [httpx]\n",
            "   ------------------------ --------------- 33/53 [httpx]\n",
            "   ------------------------ --------------- 33/53 [httpx]\n",
            "   ------------------------ --------------- 33/53 [httpx]\n",
            "   ------------------------- -------------- 34/53 [dataclasses-json]\n",
            "   -------------------------- ------------- 35/53 [banks]\n",
            "   -------------------------- ------------- 35/53 [banks]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 36/53 [openai]\n",
            "   --------------------------- ------------ 37/53 [llama-index-workflows]\n",
            "   --------------------------- ------------ 37/53 [llama-index-workflows]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ---------------------------- ----------- 38/53 [llama-cloud]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ----------------------------- ---------- 39/53 [llama-index-core]\n",
            "   ------------------------------ --------- 40/53 [llama-index-readers-file]\n",
            "   ------------------------------ --------- 40/53 [llama-index-readers-file]\n",
            "   ------------------------------ --------- 40/53 [llama-index-readers-file]\n",
            "   ------------------------------ --------- 40/53 [llama-index-readers-file]\n",
            "   ------------------------------ --------- 41/53 [llama-index-llms-openai]\n",
            "   ---------------------- ----- 42/53 [llama-index-indices-managed-llama-cloud]\n",
            "   --------------------------------- ------ 44/53 [llama-cloud-services]\n",
            "   --------------------------------- ------ 44/53 [llama-cloud-services]\n",
            "   --------------------------------- ------ 44/53 [llama-cloud-services]\n",
            "   --------------------------------- ------ 45/53 [llama-parse]\n",
            "   --------------------------------- ------ 45/53 [llama-parse]\n",
            "   ----------------------------------- ---- 47/53 [llama-index-cli]\n",
            "   ----------------------------------- ---- 47/53 [llama-index-cli]\n",
            "   ------------------------------------ --- 48/53 [llama-index-agent-openai]\n",
            "   ------------------------------------- -- 50/53 [llama-index-program-openai]\n",
            "   ---------------------------------------  52/53 [llama_index]\n",
            "   ---------------------------------------- 53/53 [llama_index]\n",
            "\n",
            "Successfully installed aiosqlite-0.21.0 annotated-types-0.7.0 anyio-4.9.0 banks-2.1.3 beautifulsoup4-4.13.4 click-8.2.1 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 distro-1.9.0 filetype-1.2.0 greenlet-3.2.3 griffe-1.7.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.10.0 llama-cloud-0.1.30 llama-cloud-services-0.6.41 llama-index-agent-openai-0.4.12 llama-index-cli-0.4.3 llama-index-core-0.12.46 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.8 llama-index-instrumentation-0.2.0 llama-index-llms-openai-0.4.7 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-index-workflows-1.0.1 llama-parse-0.6.41 llama_index-0.12.46 marshmallow-3.26.1 mypy-extensions-1.1.0 nltk-3.9.1 openai-1.93.0 pandas-2.2.3 pydantic-2.11.7 pydantic-core-2.33.2 pypdf-5.7.0 python-dotenv-1.1.1 setuptools-80.9.0 sniffio-1.3.1 soupsieve-2.7 sqlalchemy-2.0.41 striprtf-0.0.26 tenacity-9.1.2 tiktoken-0.9.0 typing-inspect-0.9.0 typing-inspection-0.4.1 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "!pip install llama_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\EIL\\embeddings\\.venv\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:628: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import LLM\n",
        "from llama_index.core.llms import CompletionResponse\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "class HuggingFacePhi3LLM(LLM):\n",
        "    tokenizer: any = None\n",
        "    model: any = None\n",
        "    generator: any = None\n",
        "\n",
        "    def __init__(self, model_name=\"microsoft/phi-3-mini-4k-instruct\", device=\"cuda\"):\n",
        "        object.__setattr__(self, 'tokenizer', AutoTokenizer.from_pretrained(model_name))\n",
        "        object.__setattr__(self, 'model', AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\"))\n",
        "        object.__setattr__(self, 'generator', pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer))\n",
        "\n",
        "    def complete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
        "        response = self.generator(prompt, max_new_tokens=128, do_sample=False)[0][\"generated_text\"]\n",
        "        # Strip prompt from response if model echoes it\n",
        "        answer = response.replace(prompt, \"\").strip()\n",
        "        return CompletionResponse(text=answer)\n",
        "\n",
        "    def achat(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def acomplete(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def astream_chat(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def astream_complete(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def chat(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        return {}\n",
        "\n",
        "    def stream_chat(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def stream_complete(self, *args, **kwargs):\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "def load_corpus(files, verbose=False):\n",
        "    if verbose:\n",
        "        print(f\"Loading files {files}\")\n",
        "\n",
        "    reader = SimpleDirectoryReader(input_files=files)\n",
        "    docs = reader.load_data()\n",
        "    if verbose:\n",
        "        print(f\"Loaded {len(docs)} docs\")\n",
        "\n",
        "    parser = SentenceSplitter()\n",
        "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Parsed {len(nodes)} nodes\")\n",
        "\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-finetuning\n",
            "  Downloading llama_index_finetuning-0.3.2-py3-none-any.whl.metadata (992 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-finetuning) (0.12.46)\n",
            "Collecting llama-index-embeddings-adapter<0.4.0,>=0.3.0 (from llama-index-finetuning)\n",
            "  Downloading llama_index_embeddings_adapter-0.3.0-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-llms-azure-openai<0.4.0,>=0.3.0 (from llama-index-finetuning)\n",
            "  Downloading llama_index_llms_azure_openai-0.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-llms-mistralai<0.5.0,>=0.4.0 (from llama-index-finetuning)\n",
            "  Downloading llama_index_llms_mistralai-0.4.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0 (from llama-index-finetuning)\n",
            "  Downloading llama_index_postprocessor_cohere_rerank-0.3.0-py3-none-any.whl.metadata (721 bytes)\n",
            "Requirement already satisfied: sentence-transformers>=2.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-finetuning) (5.0.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.12.13)\n",
            "Requirement already satisfied: aiosqlite in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2025.3.0)\n",
            "Requirement already satisfied: httpx in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.0.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.9.1)\n",
            "Requirement already satisfied: numpy in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2.2.6)\n",
            "Requirement already satisfied: pillow>=9.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (11.3.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.9.0)\n",
            "Requirement already satisfied: wrapt in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.20.1)\n",
            "Requirement already satisfied: griffe in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (4.3.8)\n",
            "Requirement already satisfied: torch>=2.0.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-embeddings-adapter<0.4.0,>=0.3.0->llama-index-finetuning) (2.7.1)\n",
            "Collecting azure-identity<2,>=1.15.0 (from llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Using cached azure_identity-1.23.0-py3-none-any.whl.metadata (81 kB)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (0.4.7)\n",
            "Collecting azure-core>=1.31.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Downloading azure_core-1.35.0-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting cryptography>=2.5 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Downloading cryptography-45.0.5-cp37-abi3-win_amd64.whl.metadata (5.7 kB)\n",
            "Collecting msal>=1.30.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Using cached msal-1.32.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting mistralai>=1.0.0 (from llama-index-llms-mistralai<0.5.0,>=0.4.0->llama-index-finetuning)\n",
            "  Downloading mistralai-1.9.1-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: openai<2,>=1.81.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (1.93.0)\n",
            "Collecting cohere<6.0.0,>=5.1.1 (from llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Downloading cohere-5.15.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Downloading fastavro-1.11.1-cp310-cp310-win_amd64.whl.metadata (5.7 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning) (2.33.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning) (0.21.2)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.2.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (0.10.0)\n",
            "Requirement already satisfied: sniffio in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2,>=1.81.0->llama-index-llms-openai<0.5,>=0.4.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (3.10)\n",
            "Requirement already satisfied: certifi in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tokenizers<1,>=0.15->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning) (0.33.2)\n",
            "Requirement already satisfied: filelock in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.9 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.4.0,>=0.3.0->llama-index-finetuning) (25.0)\n",
            "Requirement already satisfied: colorama in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (0.4.6)\n",
            "Requirement already satisfied: six>=1.11.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from azure-core>=1.31.0->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning) (1.17.0)\n",
            "Collecting cffi>=1.14 (from cryptography>=2.5->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
            "Collecting pycparser (from cffi>=1.14->cryptography>=2.5->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.0.0->llama-index-llms-mistralai<0.5.0,>=0.4.0->llama-index-finetuning)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from mistralai>=1.0.0->llama-index-llms-mistralai<0.5.0,>=0.4.0->llama-index-finetuning) (2.9.0.post0)\n",
            "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-finetuning)\n",
            "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: click in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (8.2.1)\n",
            "Requirement already satisfied: joblib in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (2024.11.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers>=2.3.0->llama-index-finetuning) (4.53.0)\n",
            "Requirement already satisfied: scikit-learn in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers>=2.3.0->llama-index-finetuning) (1.7.0)\n",
            "Requirement already satisfied: scipy in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sentence-transformers>=2.3.0->llama-index-finetuning) (1.15.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.3.0->llama-index-finetuning) (0.5.3)\n",
            "Requirement already satisfied: greenlet>=1 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.2.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from torch>=2.0.0->llama-index-embeddings-adapter<0.4.0,>=0.3.0->llama-index-finetuning) (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->llama-index-embeddings-adapter<0.4.0,>=0.3.0->llama-index-finetuning) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.26.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-finetuning) (3.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\eil\\embeddings\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.3.0->llama-index-finetuning) (3.6.0)\n",
            "Downloading llama_index_finetuning-0.3.2-py3-none-any.whl (32 kB)\n",
            "Downloading llama_index_embeddings_adapter-0.3.0-py3-none-any.whl (4.5 kB)\n",
            "Downloading llama_index_llms_azure_openai-0.3.4-py3-none-any.whl (7.3 kB)\n",
            "Using cached azure_identity-1.23.0-py3-none-any.whl (186 kB)\n",
            "Downloading llama_index_llms_mistralai-0.4.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading llama_index_postprocessor_cohere_rerank-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
            "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.11.1-cp310-cp310-win_amd64.whl (449 kB)\n",
            "Downloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading azure_core-1.35.0-py3-none-any.whl (210 kB)\n",
            "Downloading cryptography-45.0.5-cp37-abi3-win_amd64.whl (3.4 MB)\n",
            "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 0.5/3.4 MB 1.5 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 0.8/3.4 MB 1.7 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 1.3/3.4 MB 1.9 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 1.8/3.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 2.6/3.4 MB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.4/3.4 MB 2.5 MB/s eta 0:00:00\n",
            "Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
            "Downloading mistralai-1.9.1-py3-none-any.whl (381 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Using cached msal-1.32.3-py3-none-any.whl (115 kB)\n",
            "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
            "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "Installing collected packages: types-requests, PyJWT, pycparser, httpx-sse, fastavro, eval-type-backport, cffi, azure-core, cryptography, mistralai, cohere, msal, msal-extensions, llama-index-postprocessor-cohere-rerank, llama-index-llms-mistralai, llama-index-embeddings-adapter, azure-identity, llama-index-llms-azure-openai, llama-index-finetuning\n",
            "\n",
            "   -- -------------------------------------  1/19 [PyJWT]\n",
            "   -- -------------------------------------  1/19 [PyJWT]\n",
            "   ---- -----------------------------------  2/19 [pycparser]\n",
            "   ---- -----------------------------------  2/19 [pycparser]\n",
            "   ---- -----------------------------------  2/19 [pycparser]\n",
            "   ------ ---------------------------------  3/19 [httpx-sse]\n",
            "   -------- -------------------------------  4/19 [fastavro]\n",
            "   -------- -------------------------------  4/19 [fastavro]\n",
            "   -------- -------------------------------  4/19 [fastavro]\n",
            "   -------- -------------------------------  4/19 [fastavro]\n",
            "   -------- -------------------------------  4/19 [fastavro]\n",
            "   -------- -------------------------------  4/19 [fastavro]\n",
            "   ---------- -----------------------------  5/19 [eval-type-backport]\n",
            "   ------------ ---------------------------  6/19 [cffi]\n",
            "   ------------ ---------------------------  6/19 [cffi]\n",
            "   ------------ ---------------------------  6/19 [cffi]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   -------------- -------------------------  7/19 [azure-core]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ---------------- -----------------------  8/19 [cryptography]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   ------------------ ---------------------  9/19 [mistralai]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   --------------------- ------------------ 10/19 [cohere]\n",
            "   ----------------------- ---------------- 11/19 [msal]\n",
            "   ----------------------- ---------------- 11/19 [msal]\n",
            "   ----------------------- ---------------- 11/19 [msal]\n",
            "   ----------------------- ---------------- 11/19 [msal]\n",
            "   ------------------------- -------------- 12/19 [msal-extensions]\n",
            "   ----------------------------- ---------- 14/19 [llama-index-llms-mistralai]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   --------------------------------- ------ 16/19 [azure-identity]\n",
            "   ------------------------------------- -- 18/19 [llama-index-finetuning]\n",
            "   ------------------------------------- -- 18/19 [llama-index-finetuning]\n",
            "   ------------------------------------- -- 18/19 [llama-index-finetuning]\n",
            "   ---------------------------------------- 19/19 [llama-index-finetuning]\n",
            "\n",
            "Successfully installed PyJWT-2.10.1 azure-core-1.35.0 azure-identity-1.23.0 cffi-1.17.1 cohere-5.15.0 cryptography-45.0.5 eval-type-backport-0.2.2 fastavro-1.11.1 httpx-sse-0.4.0 llama-index-embeddings-adapter-0.3.0 llama-index-finetuning-0.3.2 llama-index-llms-azure-openai-0.3.4 llama-index-llms-mistralai-0.4.0 llama-index-postprocessor-cohere-rerank-0.3.0 mistralai-1.9.1 msal-1.32.3 msal-extensions-1.3.1 pycparser-2.22 types-requests-2.32.4.20250611\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edf1010e7b314d23917da3199a5ff5a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
            "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
            "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "  0%|          | 0/154 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "  1%|          | 1/154 [14:58<38:11:50, 898.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "  1%|          | 1/154 [27:15<69:30:01, 1635.30s/it]\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m phi3_llm \u001b[38;5;241m=\u001b[39m HuggingFacePhi3LLM()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Use it for QA pair generation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_qa_embedding_pairs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphi3_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_dataset.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m generate_qa_embedding_pairs(\n\u001b[0;32m     19\u001b[0m     llm\u001b[38;5;241m=\u001b[39mphi3_llm,\n\u001b[0;32m     20\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mval_nodes,\n\u001b[0;32m     21\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_dataset.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Optional: load into dataset objects\u001b[39;00m\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\llama_index\\finetuning\\embeddings\\common.py:144\u001b[0m, in \u001b[0;36mgenerate_qa_embedding_pairs\u001b[1;34m(nodes, llm, qa_generate_prompt_tmpl, num_questions_per_chunk, retry_limit, on_failure, save_every, output_path, verbose)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m retry_limit:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:319\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
            "Cell \u001b[1;32mIn[29], line 16\u001b[0m, in \u001b[0;36mHuggingFacePhi3LLM.complete\u001b[1;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcomplete\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponse:\n\u001b[1;32m---> 16\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Strip prompt from response if model echoes it\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     answer \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:316\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1464\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1457\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1458\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1461\u001b[0m         )\n\u001b[0;32m   1462\u001b[0m     )\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1471\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1470\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1471\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1472\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1371\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1370\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1371\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1372\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:414\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    412\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 414\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[0;32m    417\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:2623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2615\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2616\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2617\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2618\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2620\u001b[0m     )\n\u001b[0;32m   2622\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2623\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2624\u001b[0m         input_ids,\n\u001b[0;32m   2625\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2626\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2627\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2628\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2629\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2630\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2631\u001b[0m     )\n\u001b[0;32m   2633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2634\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2635\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2636\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2637\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2638\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2639\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2640\u001b[0m     )\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:3607\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3605\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3607\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3609\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3610\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3611\u001b[0m     outputs,\n\u001b[0;32m   3612\u001b[0m     model_kwargs,\n\u001b[0;32m   3613\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3614\u001b[0m )\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:584\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    580\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    581\u001b[0m )\n\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 584\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    585\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    586\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    587\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    588\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    589\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    590\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    591\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    592\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    593\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    595\u001b[0m )\n\u001b[0;32m    597\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:472\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    470\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 472\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    473\u001b[0m     hidden_states,\n\u001b[0;32m    474\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    475\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    476\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    477\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    478\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    479\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    480\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    482\u001b[0m )\n\u001b[0;32m    484\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:302\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    301\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 302\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_mlp_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[0;32m    305\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:66\u001b[0m, in \u001b[0;36mPhi3MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m     63\u001b[0m gate, up_states \u001b[38;5;241m=\u001b[39m up_states\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     64\u001b[0m up_states \u001b[38;5;241m=\u001b[39m up_states \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(gate)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_states\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32me:\\EIL\\embeddings\\.venv\\lib\\site-packages\\accelerate\\hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from llama_index.finetuning import generate_qa_embedding_pairs\n",
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
        "\n",
        "# Load your text chunks (nodes) as before\n",
        "train_nodes = load_corpus([\"mouch.pdf\"])\n",
        "val_nodes = load_corpus([\"Production and Operations Management Systems.pdf\"])\n",
        "\n",
        "# Instantiate your HF-based LLM\n",
        "phi3_llm = HuggingFacePhi3LLM()\n",
        "\n",
        "# Use it for QA pair generation\n",
        "train_dataset = generate_qa_embedding_pairs(\n",
        "    llm=phi3_llm,\n",
        "    nodes=train_nodes,\n",
        "    output_path=\"train_dataset.json\"\n",
        ")\n",
        "\n",
        "val_dataset = generate_qa_embedding_pairs(\n",
        "    llm=phi3_llm,\n",
        "    nodes=val_nodes,\n",
        "    output_path=\"val_dataset.json\"\n",
        ")\n",
        "\n",
        "# Optional: load into dataset objects\n",
        "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
        "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5l3G4kaGH1Ap"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.evaluation import (\n",
        "    InformationRetrievalEvaluator,\n",
        "    SequentialEvaluator,\n",
        ")\n",
        "from sentence_transformers.util import cos_sim\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "model_id = \"BAAI/bge-small-en-v1.5\"  # Hugging Face model ID\n",
        "matryoshka_dimensions = [256, 128, 64] # Important: large to small\n",
        "\n",
        "# Load a model\n",
        "model = SentenceTransformer(\n",
        "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# load test dataset from json using pandas\n",
        "test_df = pd.read_json(\"test_dataset.json\", orient=\"records\", lines=True)\n",
        "train_df = pd.read_json(\"train_dataset.json\", orient=\"records\", lines=True)\n",
        "\n",
        "# Convert pandas DataFrames to Hugging Face Datasets\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "# Convert the datasets to dictionaries\n",
        "corpus = dict(\n",
        "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
        ")  # Our corpus (cid => document)\n",
        "queries = dict(\n",
        "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
        ")  # Our queries (qid => question)\n",
        "\n",
        "# Create a mapping of relevant document (1 in our case) to each query\n",
        "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
        "for q_id in queries:\n",
        "    relevant_docs[q_id] = [q_id]\n",
        "\n",
        "\n",
        "matryoshka_evaluators = []\n",
        "# Iterate over the different dimensions\n",
        "for dim in matryoshka_dimensions:\n",
        "    ir_evaluator = InformationRetrievalEvaluator(\n",
        "        queries=queries,\n",
        "        corpus=corpus,\n",
        "        relevant_docs=relevant_docs,\n",
        "        name=f\"dim_{dim}\",\n",
        "        truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
        "        score_functions={\"cosine\": cos_sim},\n",
        "    )\n",
        "    matryoshka_evaluators.append(ir_evaluator)\n",
        "\n",
        "# Create a sequential evaluator\n",
        "evaluator = SequentialEvaluator(matryoshka_evaluators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjBInt09I0Ha"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "results = evaluator(model)\n",
        "\n",
        "# # COMMENT IN for full results\n",
        "# print(results)\n",
        "\n",
        "# Print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
        "    print\n",
        "    print(f\"{key}: {results[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QMTXaQgI_1q"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerModelCardData, SentenceTransformer\n",
        "\n",
        "# Hugging Face model ID: https://huggingface.co/BAAI/bge-base-en-v1.5\n",
        "model_id = \"BAAI/bge-small-en-v1.5\"\n",
        "\n",
        "# load model with SDPA for using Flash Attention 2\n",
        "model = SentenceTransformer(\n",
        "    model_id,\n",
        "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
        "    model_card_data=SentenceTransformerModelCardData(\n",
        "        language=\"en\",\n",
        "        license=\"apache-2.0\",\n",
        "        model_name=\"BGE base Financial Matryoshka\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPmg14eKJGpN"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [256, 128, 64]  # Important: large to small\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yhJGu_GnJH81"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "\n",
        "# load train dataset again\n",
        "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
        "\n",
        "# define training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"bge-base-financial-matryoshka\", # output directory and hugging face model ID\n",
        "    num_train_epochs=4,                         # number of epochs\n",
        "    per_device_train_batch_size=8,             # train batch size\n",
        "    gradient_accumulation_steps=8,             # for a global batch size of 512\n",
        "    per_device_eval_batch_size=16,              # evaluation batch size\n",
        "    warmup_ratio=0.1,                           # warmup ratio\n",
        "    learning_rate=2e-5,                         # learning rate, 2e-5 is a good value\n",
        "    lr_scheduler_type=\"cosine\",                 # use constant learning rate scheduler\n",
        "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
        "    tf32=False,                                  # use tf32 precision\n",
        "    bf16=True,                                  # use bf16 precision\n",
        "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
        "    eval_strategy=\"epoch\",                      # evaluate after each epoch\n",
        "    save_strategy=\"epoch\",                      # save after each epoch\n",
        "    logging_steps=10,                           # log every 10 steps\n",
        "    save_total_limit=3,                         # save only the last 3 models\n",
        "    load_best_model_at_end=True,                # load the best model when training ends\n",
        "    metric_for_best_model=\"eval_dim_128_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score for the 128 dimension\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-JNnipjvJJVt"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerTrainer\n",
        "\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset.select_columns(\n",
        "        [\"positive\", \"anchor\"]\n",
        "    ),\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UEjeBdtXKyXU"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0vKuu1MzK0iz"
      },
      "outputs": [],
      "source": [
        "wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ko634OG3JKik"
      },
      "outputs": [],
      "source": [
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save the best model\n",
        "trainer.save_model()\n",
        "\n",
        "# push model to hub\n",
        "# trainer.model.push_to_hub(\"bge-small-financial-matryoshka\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3jCgQAJJL6U"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "fine_tuned_model = SentenceTransformer(\n",
        "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "# Evaluate the model\n",
        "results = evaluator(fine_tuned_model)\n",
        "\n",
        "# # COMMENT IN for full results\n",
        "# print(results)\n",
        "\n",
        "# Print the main score\n",
        "for dim in matryoshka_dimensions:\n",
        "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
        "    print(f\"{key}: {results[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iQxCX1Uk4Lo"
      },
      "outputs": [],
      "source": [
        "dim_256_cosine_ndcg@10: 0.7957342722206704\n",
        "dim_128_cosine_ndcg@10: 0.7898308785293717\n",
        "dim_64_cosine_ndcg@10: 0.7614769594274166\n",
        "\n",
        "\n",
        "dim_256_cosine_ndcg@10: 0.7319604909788028\n",
        "dim_128_cosine_ndcg@10: 0.7045758818102816\n",
        "dim_64_cosine_ndcg@10: 0.6334755429440541"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-hLQ9KC87Ey"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python (embeddings_venv)",
      "language": "python",
      "name": "embeddings_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "048a59f82a774be7bb4f437603dd325b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "269340fb19a24356a6cfbab10ceed0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2961efceeb1f4a1a8261bec721dee6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d59225bbba7409caa89d1825fa5c0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65d0240a89ae4e88a5d033637c52a061",
              "IPY_MODEL_3a22eae0be5744aab43b0042d10f94ba",
              "IPY_MODEL_e8d4e48bf7a646dd99e8c0c008f10655",
              "IPY_MODEL_d5c64e8355b3426cae610b4498360047",
              "IPY_MODEL_82a45c5911d64394935d929098227548"
            ],
            "layout": "IPY_MODEL_b0442656fcea4423ba664d06307054a8"
          }
        },
        "3a22eae0be5744aab43b0042d10f94ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_74ac21c68edc454fa7acea4e879e377b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6089020e1c9e40b799ed4bdd9e260d80",
            "value": ""
          }
        },
        "3cafc4c5f6874e25aafb6380fcdbd651": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b11611a2090408fbee0a94616efb2d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6089020e1c9e40b799ed4bdd9e260d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65d0240a89ae4e88a5d033637c52a061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff6c8edede64a4087ea3509d3e5d3bc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_269340fb19a24356a6cfbab10ceed0cc",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "74ac21c68edc454fa7acea4e879e377b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a45c5911d64394935d929098227548": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cafc4c5f6874e25aafb6380fcdbd651",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e2d701ca633647a1ab1a75b6437726d5",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "aff6c8edede64a4087ea3509d3e5d3bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0442656fcea4423ba664d06307054a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ce2e5768f2814f5d83a2afc8873607bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d5c64e8355b3426cae610b4498360047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_048a59f82a774be7bb4f437603dd325b",
            "style": "IPY_MODEL_ce2e5768f2814f5d83a2afc8873607bd",
            "tooltip": ""
          }
        },
        "e2d701ca633647a1ab1a75b6437726d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8d4e48bf7a646dd99e8c0c008f10655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_4b11611a2090408fbee0a94616efb2d2",
            "style": "IPY_MODEL_2961efceeb1f4a1a8261bec721dee6ed",
            "value": true
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
